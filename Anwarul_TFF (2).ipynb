{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scipy tensorflow tensorflow-federated==0.41 pandas scikit-learn cryptography google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLSdck3G2hHb",
        "outputId": "c394755a-86ea-4e9a-9190-78224a1a5982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.1)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting tensorflow-federated==0.41\n",
            "  Using cached tensorflow_federated-0.41.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (43.0.1)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (2.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Collecting google-auth-oauthlib\n",
            "  Using cached google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.147.0)\n",
            "Requirement already satisfied: absl-py==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated==0.41) (1.4.0)\n",
            "Collecting attrs~=21.4 (from tensorflow-federated==0.41)\n",
            "  Using cached attrs-21.4.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting cachetools~=3.1 (from tensorflow-federated==0.41)\n",
            "  Using cached cachetools-3.1.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting dm-tree==0.1.7 (from tensorflow-federated==0.41)\n",
            "  Using cached dm_tree-0.1.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting dp-accounting==0.3.0 (from tensorflow-federated==0.41)\n",
            "  Using cached dp_accounting-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting farmhashpy==0.4.0 (from tensorflow-federated==0.41)\n",
            "  Using cached farmhashpy-0.4.0.tar.gz (98 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: grpcio~=1.46 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated==0.41) (1.64.1)\n",
            "INFO: pip is looking at multiple versions of tensorflow-federated to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 0.4.32\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 0.34.0 Requires-Python ~=3.9.0; 0.36.0 Requires-Python ~=3.9.0; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement jaxlib==0.3.14 (from tensorflow-federated) (from versions: 0.4.6, 0.4.7, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13, 0.4.14, 0.4.16, 0.4.17, 0.4.18, 0.4.19, 0.4.20, 0.4.21, 0.4.22, 0.4.23, 0.4.24, 0.4.25, 0.4.26, 0.4.27, 0.4.28, 0.4.29, 0.4.30, 0.4.31, 0.4.33)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for jaxlib==0.3.14\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenseal\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azjILHr8oKG2",
        "outputId": "e219feb2-5c42-4895-c59b-da87318e1a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tenseal in /usr/local/lib/python3.10/dist-packages (0.3.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from cryptography.hazmat.backends import default_backend\n",
        "from cryptography.hazmat.primitives import hashes\n",
        "from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
        "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from google.auth.transport.requests import Request\n",
        "import google.auth\n",
        "import base64\n",
        "import tenseal as ts\n",
        "\n",
        "# Authenticate and create the Google Drive client\n",
        "print(\"Authenticating and creating Google Drive client...\")\n",
        "print(\"Client : Anwarul\")\n",
        "auth.authenticate_user()\n",
        "creds, _ = google.auth.default()\n",
        "if creds.expired and creds.refresh_token:\n",
        "    creds.refresh(Request())\n",
        "print(\"Authentication successful. Connected to Google Drive.\")\n",
        "\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "print(\"Google Drive is set as the global server.\")\n",
        "\n",
        "# Load the dataset directly from the uploaded file\n",
        "data_path = '/content/johns_hospital_data.csv'  # Change filename as needed\n",
        "print(f\"Loading dataset from {data_path}...\")\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "# Data Preprocessing\n",
        "print(\"Starting data preprocessing...\")\n",
        "data_cleaned = data.drop(['Admission_date', 'hospital_name', 'patient_id',\n",
        "                         'patient_first_initial', 'patient_last_name',\n",
        "                         'doctor_name', 'patient_checkin_date', 'patient_checkout_date'], axis=1)\n",
        "\n",
        "# Convert categorical columns to numerical using Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_columns = data_cleaned.select_dtypes(include=['object']).columns\n",
        "\n",
        "for column in categorical_columns:\n",
        "    data_cleaned[column] = label_encoder.fit_transform(data_cleaned[column])\n",
        "\n",
        "# Define features and target variable\n",
        "X = data_cleaned.drop('readmission', axis=1)\n",
        "y = data_cleaned['readmission']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"Data split into training and testing sets.\")\n",
        "\n",
        "# Standardize the dataset\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to TensorFlow Dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
        "\n",
        "# Prepare the dataset for federated learning\n",
        "def preprocess(dataset):\n",
        "    return dataset.map(lambda x, y: (tf.cast(x, tf.float32), tf.cast(y, tf.int32))) \\\n",
        "                  .map(lambda x, y: (x, tf.expand_dims(y, -1)))\n",
        "\n",
        "train_dataset = preprocess(train_dataset)\n",
        "print(\"Data preprocessing completed successfully.\")\n",
        "\n",
        "# Define the model function for Federated Learning\n",
        "def create_keras_model():\n",
        "    return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # Ensure input shape is correct\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
        "    ])\n",
        "\n",
        "def model_fn():\n",
        "    keras_model = create_keras_model()\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model,\n",
        "        input_spec=(tf.TensorSpec(shape=[None, X_train.shape[1]], dtype=tf.float32),\n",
        "                    tf.TensorSpec(shape=[None, 1], dtype=tf.int32)),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "print(\"Model function defined successfully.\")\n",
        "\n",
        "# Homomorphic Encryption Setup\n",
        "def encrypt_weights_homomorphic(weights):\n",
        "    encrypted_weights = []\n",
        "\n",
        "    print(\"Encrypting model weights using homomorphic encryption...\")\n",
        "    context = ts.context(ts.SCHEME_TYPE.CKKS, poly_modulus_degree=8192, coeff_mod_bit_sizes=[60, 40, 40, 60])\n",
        "    context.generate_galois_keys()\n",
        "    context.global_scale = 2**40\n",
        "\n",
        "    for w in weights:\n",
        "        data = w.flatten().tolist()\n",
        "        encrypted_data = ts.ckks_vector(context, data)\n",
        "        encrypted_weights.append(encrypted_data)\n",
        "\n",
        "    print(\"Model weights encrypted successfully using homomorphic encryption.\")\n",
        "    return encrypted_weights\n",
        "\n",
        "def decrypt_weights_homomorphic(encrypted_weights):\n",
        "    decrypted_weights = []\n",
        "\n",
        "    print(\"Decrypting model weights using homomorphic encryption...\")\n",
        "    for encrypted_data in encrypted_weights:\n",
        "        decrypted_data = encrypted_data.decrypt()\n",
        "        decrypted_weights.append(np.array(decrypted_data))\n",
        "\n",
        "    print(\"Model weights decrypted successfully using homomorphic encryption.\")\n",
        "    return decrypted_weights\n",
        "\n",
        "# Example of encrypting model weights using homomorphic encryption\n",
        "model_weights = create_keras_model().get_weights()\n",
        "encrypted_weights_homomorphic = encrypt_weights_homomorphic([w.flatten() for w in model_weights])\n",
        "\n",
        "# Decrypt weights when needed\n",
        "decrypted_weights_homomorphic = decrypt_weights_homomorphic(encrypted_weights_homomorphic)\n",
        "\n",
        "# Reshape the decrypted weights to match the original shapes\n",
        "decrypted_weights_homomorphic = [w.reshape(original_w.shape) for w, original_w in zip(decrypted_weights_homomorphic, model_weights)]\n",
        "print(\"Model weights encryption and decryption completed successfully using homomorphic encryption.\")\n",
        "\n",
        "# Proceed with federated learning setup\n",
        "try:\n",
        "    print(\"Building federated averaging process...\")\n",
        "    client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "\n",
        "    iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "        model_fn=model_fn,\n",
        "        client_optimizer_fn=client_optimizer_fn\n",
        "    )\n",
        "    state = iterative_process.initialize()\n",
        "    print(\"Federated averaging process initialized successfully.\")\n",
        "\n",
        "    # Indicating connection between clients and global server\n",
        "    print(\"Clients are connected to the global server for federated learning.\")\n",
        "\n",
        "    # Perform multiple rounds of federated learning\n",
        "    for round_num in range(1, 11):\n",
        "        print(f\"Starting round {round_num} of federated learning...\")\n",
        "        state, metrics = iterative_process.next(state, [train_dataset])\n",
        "        print(f'Round {round_num} metrics: {metrics}')\n",
        "\n",
        "    print(\"Federated learning rounds completed successfully.\")\n",
        "except AttributeError as ae:\n",
        "    print(\"AttributeError:\", ae)\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n",
        "\n",
        "# Save Model Updates to Google Drive in the new Keras format\n",
        "save_path = '/tmp/Anwarul_model_updates.keras'\n",
        "model_to_save = create_keras_model()  # Create the model\n",
        "model_to_save.set_weights(decrypted_weights_homomorphic)  # Set weights if using encryption\n",
        "model_to_save.save(save_path)\n",
        "print(f\"Model saved locally at {save_path}.\")\n",
        "\n",
        "file_metadata = {\n",
        "    'name': 'Anwarul_model_updates.keras',\n",
        "    'parents': ['1WVQMK27bObX0N0hDntjz1peL8bTHvsi7']  # Your global server folder ID\n",
        "}\n",
        "media = MediaFileUpload(save_path, mimetype='application/octet-stream')\n",
        "uploaded_file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "print(f\"Model saved to Google Drive with ID: {uploaded_file.get('id')}.\")\n",
        "\n",
        "# Load Model Updates\n",
        "loaded_model = tf.keras.models.load_model(save_path)\n",
        "loaded_weights = loaded_model.get_weights()\n",
        "print(\"Loaded model weights from Google Drive.\")\n",
        "\n",
        "# Comparing Loaded and Decrypted Weights\n",
        "def compare_weights(loaded_weights, decrypted_weights):\n",
        "    if len(loaded_weights) != len(decrypted_weights):\n",
        "        return False\n",
        "    for lw, dw in zip(loaded_weights, decrypted_weights):\n",
        "        if lw.shape != dw.shape or not np.allclose(lw, dw, rtol=1e-5, atol=1e-8):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "if compare_weights(loaded_weights, decrypted_weights_homomorphic):\n",
        "    print(\"Weights loaded correctly and match the saved weights.\")\n",
        "else:\n",
        "    print(\"Discrepancy found in loaded weights.\")\n",
        "\n",
        "# List files in the global server folder\n",
        "results = drive_service.files().list(\n",
        "    q=f\"'1WVQMK27bObX0N0hDntjz1peL8bTHvsi7' in parents\",\n",
        "    pageSize=10, fields=\"files(id, name)\").execute()\n",
        "items = results.get('files', [])\n",
        "\n",
        "if not items:\n",
        "    print(\"No files found in the global server folder.\")\n",
        "else:\n",
        "    print(\"Files found in the global server folder:\")\n",
        "    for item in items:\n",
        "        print(f\"{item['name']} ({item['id']})\")\n",
        "\n",
        "def privacy_measure(original_weights, encrypted_weights):\n",
        "    original_sum = sum(np.sum(w) for w in original_weights)\n",
        "    decrypted_weights = decrypt_weights_homomorphic(encrypted_weights)\n",
        "    decrypted_sum = sum(np.sum(w) for w in decrypted_weights)\n",
        "\n",
        "    if np.isclose(original_sum, decrypted_sum):\n",
        "        print(\"Privacy is preserved!!\")\n",
        "    else:\n",
        "        print(\"Privacy is compromised!!\")\n",
        "\n",
        "privacy_measure(model_weights, encrypted_weights_homomorphic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbPzIOAYUmql",
        "outputId": "a09f15a8-2532-41c7-c576-54cda7940c26"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticating and creating Google Drive client...\n",
            "Client : Anwarul\n",
            "Authentication successful. Connected to Google Drive.\n",
            "Google Drive is set as the global server.\n",
            "Loading dataset from /content/johns_hospital_data.csv...\n",
            "Starting data preprocessing...\n",
            "Data split into training and testing sets.\n",
            "Data preprocessing completed successfully.\n",
            "Model function defined successfully.\n",
            "Encrypting model weights using homomorphic encryption...\n",
            "Model weights encrypted successfully using homomorphic encryption.\n",
            "Decrypting model weights using homomorphic encryption...\n",
            "Model weights decrypted successfully using homomorphic encryption.\n",
            "Model weights encryption and decryption completed successfully using homomorphic encryption.\n",
            "Building federated averaging process...\n",
            "Federated averaging process initialized successfully.\n",
            "Clients are connected to the global server for federated learning.\n",
            "Starting round 1 of federated learning...\n",
            "Round 1 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.551625), ('loss', 0.6929706), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 2 of federated learning...\n",
            "Round 2 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.57775), ('loss', 0.6836045), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 3 of federated learning...\n",
            "Round 3 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5825), ('loss', 0.6806551), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 4 of federated learning...\n",
            "Round 4 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.584), ('loss', 0.679037), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 5 of federated learning...\n",
            "Round 5 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.585375), ('loss', 0.6779377), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 6 of federated learning...\n",
            "Round 6 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.586125), ('loss', 0.6770711), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 7 of federated learning...\n",
            "Round 7 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.587), ('loss', 0.6763246), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 8 of federated learning...\n",
            "Round 8 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.587375), ('loss', 0.6756589), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 9 of federated learning...\n",
            "Round 9 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.58875), ('loss', 0.6750459), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Starting round 10 of federated learning...\n",
            "Round 10 metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.58975), ('loss', 0.6744806), ('num_examples', 8000), ('num_batches', 250)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Federated learning rounds completed successfully.\n",
            "Model saved locally at /tmp/Anwarul_model_updates.keras.\n",
            "Model saved to Google Drive with ID: 12B9KALgxFEBsHU8PygFyG6ta4CJEFrUf.\n",
            "Loaded model weights from Google Drive.\n",
            "Weights loaded correctly and match the saved weights.\n",
            "Files found in the global server folder:\n",
            "Anwarul_model_updates.keras (12B9KALgxFEBsHU8PygFyG6ta4CJEFrUf)\n",
            "Decrypting model weights using homomorphic encryption...\n",
            "Model weights decrypted successfully using homomorphic encryption.\n",
            "Privacy is preserved!!\n"
          ]
        }
      ]
    }
  ]
}